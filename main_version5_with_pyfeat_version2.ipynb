{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc84d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chakradhar/.conda/envs/multimodular/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/chakradhar/.conda/envs/multimodular/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/chakradhar/.conda/envs/multimodular/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/chakradhar/.conda/envs/multimodular/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import cv2\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "from deepface import DeepFace\n",
    "from utility.yolov5_output_processing import processing_yolov5\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "from utility.utility_functions import write_person_data_toFile\n",
    "from utility.utility_functions import person_data_update\n",
    "\n",
    "from deepface_similarity_matching.face_similarity_matching import find_similars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336939b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from body_pose_recognition.body_features_extraction_train import body_features_extraction\n",
    "from face_emotion_recognition.face_feature_extraction_pyfeat import face_features_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e2e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def engagement_plot_hist(y, plot_width=480, plot_height=200, dpi=200):\n",
    "    x = (0, 1)\n",
    "    fig = plt.figure(dpi=dpi)\n",
    "    opacity = 0.4\n",
    "    bar_width = 0.2\n",
    "\n",
    "    plt.xlabel('Engagement')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    plt.xticks(x, ('Pos', 'Neg'), rotation=30)\n",
    "    \n",
    "    #plt.gcf().canvas.draw()\n",
    "    plt.gcf().set_size_inches(1.5, 2)\n",
    "    bar1 = plt.bar(x, y, bar_width, align='center', alpha=opacity, color='b')\n",
    "\n",
    "    # Add counts above the two bar graphs\n",
    "    for rect in bar1:\n",
    "        height = rect.get_height()\n",
    "        plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.0f}', ha='center', va='bottom')\n",
    "\n",
    "    #plt.legend()\n",
    "    plt.tight_layout()    \n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Now we can save it to a numpy array.\n",
    "    data2 = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    data2 = data2.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    #data2 = cv2.resize(data2, (plot_width, plot_height))\n",
    "    #cv2.imwrite('test2.png',data2)\n",
    "    plt.close()\n",
    "    \n",
    "    return data2\n",
    "\n",
    "def engagement_plot_line(y, z, plot_width=480, plot_height=200, dpi=100):\n",
    "    x = list(range(len(y)))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    #plt.gcf().canvas.draw()\n",
    "    #plt.gcf().set_size_inches(1.5, 2)\n",
    "    plt.plot(x, y, color='g', label='engaged')\n",
    "    #plt.plot(x, z, color='b', label='non-engaged')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()    \n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Now we can save it to a numpy array.\n",
    "    data2 = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    data2 = data2.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    #data2 = cv2.resize(data2, (plot_width, plot_height))\n",
    "    #cv2.imwrite('test2.png',data2)\n",
    "    plt.close()\n",
    "    \n",
    "    return data2\n",
    "\n",
    "def engagement_plot(x, y, plot_width=480, plot_height=200, dpi=200):\n",
    "    \n",
    "    plot_hist = engagement_plot_hist([x[-1], y[-1]], plot_width, plot_height, dpi)\n",
    "    plot_line = engagement_plot_line(x, y, plot_width, plot_height, dpi)\n",
    "    plot = np.ones((plot_hist.shape[0] + plot_line.shape[0], max(plot_line.shape[1], plot_hist.shape[1]), 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    plot[:plot_hist.shape[0], :plot_hist.shape[1], :] = plot_hist\n",
    "    plot[plot_hist.shape[0]:plot_hist.shape[0] + plot_line.shape[0], :plot_line.shape[1], :] = plot_line\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bacd67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def person_tracking_with_behavior_classification_video(input_video_path, base_path = '../project heavy/runs', \n",
    "                                                       sampling_rate = 5, time_period = 10, \n",
    "                                                       detections_len_threshold = 5):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    sampling_rate: total number of frames to take for every 1 sec\n",
    "    time_period: time in seconds after which averaging is done\n",
    "    detections_len_threshold: a track should have this minimum number of classifications to be a valid one,\n",
    "                                It is used while writing to csv.\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # \n",
    "    # output folder setup\n",
    "    #\n",
    "    if os.listdir(base_path):\n",
    "        run_num = max((int(runs[3:]) for runs in os.listdir(base_path))) + 1\n",
    "    else:\n",
    "        run_num = 1\n",
    "    output_base_path = os.path.join(base_path, 'run'+str(run_num))\n",
    "    os.mkdir(output_base_path) \n",
    "    video_path = os.path.join(output_base_path, 'result.avi')\n",
    "    \n",
    "    #\n",
    "    # Yolov5 medium\n",
    "    # \n",
    "    model_yolov5 = torch.hub.load('ultralytics/yolov5', 'yolov5m')\n",
    "    \n",
    "    #\n",
    "    # DeepSort object\n",
    "    #\n",
    "    deep_sort = DeepSort()\n",
    "    \n",
    "    #\n",
    "    # body_behavior_classifier function\n",
    "    #\n",
    "    bf_ex, num_bf = body_features_extraction()\n",
    "    \n",
    "    #\n",
    "    # face_emotion_classifier function\n",
    "    #\n",
    "    ff_ex, num_ff = face_features_extraction()\n",
    "    \n",
    "    #\n",
    "    # Engagement classification model\n",
    "    #\n",
    "    engagement_model = tf.keras.models.load_model('../body_face_dnn_model_250622')\n",
    "\n",
    "    #\n",
    "    # Data Collection\n",
    "    #\n",
    "    person_data = dict()\n",
    "    track_id_behavior = dict()\n",
    "    person_average_behavior = dict()\n",
    "    \n",
    "    #\n",
    "    # Graph plotting\n",
    "    #\n",
    "    count_engaged_li = [0]\n",
    "    count_non_engaged_li = [0]\n",
    "    \n",
    "    #\n",
    "    # video input and setting up the output video\n",
    "    #\n",
    "    vid = cv2.VideoCapture(input_video_path)\n",
    "    codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    vid_width, vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    out_width, out_height = 1280, 600\n",
    "    out = cv2.VideoWriter(video_path, codec, 1, (out_width, out_height))\n",
    "    \n",
    "    frames_in_seconds = sampling_rate;\n",
    "    sampling_rate = int(vid_fps // sampling_rate)\n",
    "\n",
    "    print(\"Analyzing Video: {}, \\nSampling Rate: {}\".format(video_path, sampling_rate))\n",
    "    print(vid_fps)\n",
    "    count_frame_classified = 0\n",
    "    iteration_ = 0\n",
    "    frame_num_li = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        _, img = vid.read()\n",
    "        if img is None:\n",
    "            print('Completed')\n",
    "            break\n",
    "\n",
    "        iteration_ += 1\n",
    "        \n",
    "        #for sampling of frames\n",
    "        if iteration_ % sampling_rate != 0:\n",
    "            continue\n",
    "            \n",
    "        count_frame_classified += 1\n",
    "        frame_num_li.append(iteration_)\n",
    "        \n",
    "        img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_rgb = img_in.copy()\n",
    "        img_write = img.copy()\n",
    "\n",
    "        results_yolov5 = model_yolov5(img_in, size=640)\n",
    "        names, scores, converted_boxes, converted_boxes_mobile = processing_yolov5(results_yolov5)\n",
    "\n",
    "        #\n",
    "        # Apply the DeepSort and get the tracker object\n",
    "        #\n",
    "        deep_sort.deep_sort_module(img, names, scores, converted_boxes)\n",
    "        \n",
    "        count_valid_tracks = 0\n",
    "        for track in deep_sort.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update >1:\n",
    "                continue\n",
    "            count_valid_tracks += 1    \n",
    "        if count_valid_tracks == 0:\n",
    "            continue\n",
    "        \n",
    "        #\n",
    "        # Extract the body features\n",
    "        #\n",
    "        body_id, b_features, img_openpose = bf_ex(img, deep_sort.tracker, converted_boxes_mobile)\n",
    "        b_features_dict = {}\n",
    "        for idx, Id in enumerate(body_id):\n",
    "            b_features_dict[Id] = idx\n",
    "            \n",
    "        img = img_openpose\n",
    "       \n",
    "        #\n",
    "        # Extract the face features\n",
    "        #\n",
    "        face_id, f_features = ff_ex(img_rgb, deep_sort.tracker)\n",
    "        f_features_dict = {}\n",
    "        for idx, Id in enumerate(face_id):\n",
    "            f_features_dict[Id] = idx\n",
    "        \n",
    "        frame_level_person_behavior = {}\n",
    "        #\n",
    "        # Frame level Classification\n",
    "        #\n",
    "        for track in deep_sort.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update >1:\n",
    "                continue\n",
    "            Id = track.track_id\n",
    "            flag = True\n",
    "            features = np.zeros((1, num_bf + num_ff))\n",
    "            \n",
    "            if Id in body_id and Id in face_id:\n",
    "                features[0,:num_bf] = b_features[b_features_dict[Id]]\n",
    "                features[0,num_bf:] = f_features[f_features_dict[Id]]\n",
    "                \n",
    "            elif Id in body_id:\n",
    "                features[0,:num_bf] = b_features[b_features_dict[Id]]\n",
    "            \n",
    "            elif Id in face_id:\n",
    "                features[0,num_bf:] = f_features[f_features_dict[Id]]\n",
    "            \n",
    "            else:\n",
    "                flag = False\n",
    "            x1, y1, x2, y2 = [int(coord) for coord in track.to_tlbr()]    \n",
    "            if flag:\n",
    "                #print(features.shape)\n",
    "                engagement_level = engagement_model.predict(features, verbose=False)[0]\n",
    "                #print(engagement_level)\n",
    "                if track_id_behavior.get(Id, None) is None:\n",
    "                    track_id_behavior[Id] = list()\n",
    "                track_id_behavior[Id].append(1 if engagement_level > 0.5 else 0)\n",
    "                frame_level_behavior = 'pos' if engagement_level > 0.5 else 'neg'\n",
    "                frame_level_person_behavior[Id] = frame_level_behavior\n",
    "                # drawing the bbox to the frame and frame level classification\n",
    "                if frame_level_behavior == 'pos':\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2),(0,255,0), 2)\n",
    "                    cv2.putText(img, \"eng. lev.: \" + frame_level_behavior +\"- ID: \"+str(track.track_id), \n",
    "                                (x1, y1-10), 0, 0.75, (255, 255, 255), 2)\n",
    "                else:\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), (0,0,255), 2)\n",
    "                    cv2.putText(img, \"eng. lev.: \" + frame_level_behavior +\"- ID: \"+str(track.track_id), \n",
    "                                (x1, y1-10), 0, 0.75, (255, 255, 255), 2)\n",
    "            else:\n",
    "                # if the classification is undefined\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 2)\n",
    "                cv2.putText(img, \"eng. lev.: \" + 'undefined' +\"- ID: \"+str(track.track_id), \n",
    "                            (x1, y1-10), 0, 0.75, (255, 255, 255), 2)\n",
    "                \n",
    "                \n",
    "        #\n",
    "        # Averaging the Classification\n",
    "        #\n",
    "        count_engaged, count_non_engaged = 0, 0\n",
    "        if count_frame_classified == time_period * frames_in_seconds:\n",
    "            flag_tmp = True\n",
    "            for id_, data_ in track_id_behavior.items():\n",
    "                cntr = Counter(data_)\n",
    "                if cntr.most_common(1)[0][1] == len(person_average_behavior) // 2:\n",
    "                    person_average_behavior[id_] = 'Not Defined'\n",
    "                else:\n",
    "                    if cntr.most_common(1)[0][0] == 1:\n",
    "                        person_average_behavior[id_] = 'pos'\n",
    "                        count_engaged += 1\n",
    "                    else:\n",
    "                        person_average_behavior[id_] = 'neg'\n",
    "                        count_non_engaged += 1\n",
    "            count_engaged_li.append(count_engaged)\n",
    "            count_non_engaged_li.append(count_non_engaged)  \n",
    "            #\n",
    "            # Updating the trackId data in person_data\n",
    "            #\n",
    "            person_data_update(person_data, img_write, deep_sort.tracker, iteration_, person_average_behavior)\n",
    "            \n",
    "            #\n",
    "            # resetting the dictionaries and values\n",
    "            #\n",
    "            count_frame_classified = 0\n",
    "            person_average_behavior = {}\n",
    "            track_id_behavior = {}\n",
    "            \n",
    "        #\n",
    "        # Frame level Classification into CSV\n",
    "        #\n",
    "        else:\n",
    "            person_data_update(person_data, img_write, deep_sort.tracker, iteration_, frame_level_person_behavior)\n",
    "            \n",
    "        #\n",
    "        # plotting section\n",
    "        #\n",
    "        \n",
    "        #\n",
    "        # combining image frame with plot\n",
    "        #\n",
    "        plot = engagement_plot(count_engaged_li, count_non_engaged_li)\n",
    "\n",
    "        new_frame_width, new_frame_height = img.shape[1] + plot.shape[1], max(img.shape[0], plot.shape[0])\n",
    "        new_frame = np.ones((new_frame_height, new_frame_width, 3), dtype=np.uint8) * 255\n",
    "        new_frame[:img.shape[0],:img.shape[1], :] = img\n",
    "        new_frame[:plot.shape[0], img.shape[1]:new_frame_width, :] = plot\n",
    "\n",
    "        # resizing of the new_frame as per output video specifications\n",
    "        new_frame = cv2.resize(new_frame, (out_width, out_height))\n",
    "        # show the output frame\n",
    "        out.write(new_frame)\n",
    "\n",
    "        # duplicating the classified frame so that it stays for some time in\n",
    "        # the video\n",
    "        if count_engaged + count_non_engaged > 0:\n",
    "            for i in range(4):\n",
    "                out.write(new_frame)\n",
    "\n",
    "\n",
    "#         if count_frame_classified % 10 == 0:\n",
    "#             print(\"\\tFrame classified: \", count_frame_classified)\n",
    "\n",
    "                 \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    print(\"Total Time taken: \", time.time() - start_time)\n",
    "    vid.release()\n",
    "    out.release()\n",
    "\n",
    "    #\n",
    "    # write data to File\n",
    "    #\n",
    "    write_person_data_toFile(person_data, output_base_path, frame_num_li, detections_len_threshold)\n",
    "    print(\"Video at: \", video_path)\n",
    "    \n",
    "    #\n",
    "    # Merging the Multiple tracks of same person\n",
    "    #\n",
    "    #print(find_similars(DeepFace, output_base_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a8a5e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chakradhar/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2022-6-15 Python-3.9.12 torch-1.9.0+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Video: ../project heavy/runs/run32/result.avi, \n",
      "Sampling Rate: 5\n",
      "25\n",
      "Completed\n",
      "Total Time taken:  85.95354127883911\n",
      "Images and Data written at ../project heavy/runs/run32\n",
      "Video at:  ../project heavy/runs/run32/result.avi\n"
     ]
    }
   ],
   "source": [
    "#person_tracking_with_behavior_classification_video('../project heavy/data/video/class_video1.mp4')\n",
    "person_tracking_with_behavior_classification_video(r'../person_7_20s.MOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379a370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abb82c6e",
   "metadata": {},
   "source": [
    "## Face Similarity to Find Duplicates using DeepFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800a9869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_face_weights.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/vgg_face_weights.h5\n",
      "To: /home/chakradhar/.deepface/weights/vgg_face_weights.h5\n",
      "100%|████████████████████████████████████████| 580M/580M [01:36<00:00, 6.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Representations stored in  ../project heavy/runs/run6/image / representations_vgg_face.pkl  file. Please delete this file when you add new identities in your database.\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "find function lasts  99.29092693328857  seconds\n",
      "[['7', '1', '6', '4', '3'], ['8'], ['2', '5']]\n"
     ]
    }
   ],
   "source": [
    "print(find_similars(DeepFace, '../project heavy/runs/run6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a69bd9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "backends = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface', 'mediapipe']\n",
    "\n",
    "#face verification\n",
    "obj = DeepFace.verify(img1_path = \"../project heavy/runs/run6/image/person_7.jpg\", img2_path = \"../project heavy/runs/run6/image/person_6.jpg\", detector_backend = backends[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9839435d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verified': True,\n",
       " 'distance': 0.3115096947027909,\n",
       " 'threshold': 0.4,\n",
       " 'model': 'VGG-Face',\n",
       " 'detector_backend': 'retinaface',\n",
       " 'similarity_metric': 'cosine'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27155c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
